{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gab Daos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from dateutil import parser\n",
    "import re\n",
    "from geotext import GeoText\n",
    "\n",
    "# Spacy for tokenizing our texts\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Gensim is needed for modeling\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Setting up Spacy Tokenizer\n",
    "nlp = English()\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# This will add pipelines in our tokenization process.\n",
    "\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is a function that will create a model that predicts the topics conveyed by each group of tweet sentiments\n",
    "\n",
    "\n",
    "def topic_modeler(tokenized_texts, no_topics, no_words):\n",
    "    topics = []\n",
    "\n",
    "    words = corpora.Dictionary(tokenized_texts)\n",
    "    corpus = [words.doc2bow(doc) for doc in tokenized_texts]\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=words,\n",
    "                                                random_state = 3,\n",
    "                                               num_topics= no_topics)\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_texts, dictionary=words, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rappler_scraping.csv')\n",
    "df = df.iloc[:,1:]\n",
    "df['date'] = [parser.parse(date).strftime('%Y-%m-%d') for date in df['date']]\n",
    "df = df[(df['text'].str.contains('coronavirus'))]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "#location = pd.read_csv('ph_locations.csv')\n",
    "#location = location.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Topics \n",
    "\n",
    "words = df['text'].str.lower()\n",
    "listWords = []\n",
    "for item in words:\n",
    "    listWords.append([nlp(item)])\n",
    "\n",
    "topics = []\n",
    "for x in listWords:\n",
    "    res = topic_modeler(x, 1, 30)\n",
    "    res = res.show_topic(0, topn = 30)\n",
    "    topics.append([word[0] for word in res])\n",
    "    \n",
    "df['LDA_Topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the counting phrases in the articles\n",
    "\n",
    "df['count_docs'] =  df['text'].apply(lambda x: re.findall(\"(?:[a-zA-Z'-]+[^a-zA-Z'-]+){0,0}[0-9](?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,1}\", x))\n",
    "\n",
    "checker = ['confirmed','suspected','quarantine','case','infected','monitoring','chinese']\n",
    "\n",
    "count_docs = []\n",
    "for index, row in df.iterrows():\n",
    "    passed = []\n",
    "    for item in row['count_docs']:\n",
    "        if any(ext in item.lower() for ext in checker):\n",
    "            passed.append(item)\n",
    "            break\n",
    "    \n",
    "    count_docs.append(passed)\n",
    "\n",
    "df['count_docs'] = count_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the PH Locations using geotext on the articles\n",
    "\n",
    "df['PH_Loc'] = [list(set(GeoText(content, 'PH').cities)) for content in df['text']]\n",
    "df['PH_Loc'] = [[x.lower() for x in w] for w in df['PH_Loc']]\n",
    "df['PH_Loc'] =[[x.replace('city', '') for x in w] for w in df['PH_Loc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying which articles are about suspicious or confirmed cases of the virus\n",
    "\n",
    "status = []\n",
    "for index, row in df.iterrows():\n",
    "    if ('confirmed' in row['LDA_Topics']) & ('confirm' in row['title'])  & (row['date'] >= '2020-01-30'):\n",
    "        status.append('confirmed')\n",
    "    elif ('confirmed' in row['LDA_Topics']) & (row['date'] >= '2020-01-30'):\n",
    "        status.append('confirmed')\n",
    "    elif (any(words in row['LDA_Topics']  for words in ['suspected','quarantine','case','infected','monitoring']))& ('FACT CHECK' not in row['title']) & ('FALSE' not in row['title']):\n",
    "        status.append('suspected')\n",
    "    else:\n",
    "        status.append('')\n",
    "df['status'] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Provinces in the identified locations\n",
    "\n",
    "df['PH_Loc'] = [list(set(loc) & set(location['Pro_Name'].unique())) for loc in df['PH_Loc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For locations not identified through the text, it will check with the LDA topics if a location is identified and use it instead\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if len(row['PH_Loc']) == 0:\n",
    "        try:\n",
    "            df.loc[index, 'PH_Loc'] = [list(set(row['LDA_Topics']) & set(location['Pro_Name'].unique()))]\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the document counts to just numbers\n",
    "\n",
    "counts = []\n",
    "for count in df['count_docs']:\n",
    "    try:\n",
    "        counts.append(count[0].split(' ')[0])\n",
    "    except IndexError:\n",
    "        counts.append(0)\n",
    "\n",
    "df['counts'] = counts\n",
    "df['counts'] = [str(count).replace(',', '') for count in df['counts']]\n",
    "df['counts'] = [str(count).replace('.', '') for count in df['counts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing Locations\n",
    "\n",
    "ph_loc = []\n",
    "for loc in df['PH_Loc']:\n",
    "    try:\n",
    "        ph_loc.append(loc[0])\n",
    "    except IndexError:\n",
    "        ph_loc.append('')\n",
    "df['Loc'] = ph_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing for CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)\n",
    "df.to_csv('rappler_parsed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[['source_id','date','category','title','author','text','status','counts','Loc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov = pd.read_csv('provinces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(prov, left_on='Loc', right_on = 'Pro_Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[['source_id','date','category','title','author','text','status','counts','Loc','coordinates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('rappler_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('sample_output.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
